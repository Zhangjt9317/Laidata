{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder_proj1_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhangjt9317/Laidata/blob/master/autoencoder_proj1_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4mRyqytAw8N",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset\n",
        "\n",
        "the dataset can be found under /content/autoencoder/data/ml-1m"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7miOF7BSAobx",
        "colab_type": "code",
        "outputId": "339469fb-1dcf-4857-accf-3f5b6e9c4a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!git clone https://github.com/tonylaioffer/autoencoder.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'autoencoder'...\n",
            "remote: Enumerating objects: 174, done.\u001b[K\n",
            "remote: Total 174 (delta 0), reused 0 (delta 0), pack-reused 174\u001b[K\n",
            "Receiving objects: 100% (174/174), 17.58 MiB | 15.30 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GihzZljmOafi",
        "colab_type": "code",
        "outputId": "8c2582e6-d690-4916-e4d2-1b9acf54d275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiDukf2sBoZQ",
        "colab_type": "text"
      },
      "source": [
        "## Define data process methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y62Iea2bB5hz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "def _get_training_data(FLAGS):  \n",
        "    ''' Buildind the input pipeline for training and inference using TFRecords files.\n",
        "    @return data only for the training\n",
        "    @return data for the inference\n",
        "    '''\n",
        "    '''\n",
        "    I guess here it's not read the entire dataset into memory in one time, and this is\n",
        "    a lazy operation, need session to activate it, so in this operation, it rules certain\n",
        "    actions in series:\n",
        "    -create TFRecordDataset to read files\n",
        "    -map this binary TFRecord dataset to contains feature 'movie_ratings'\n",
        "    -shuffle it to randomly extract 500 in buffer each time\n",
        "    -repeat this action infinite times ( i guess it's would end while all data are processed)\n",
        "    -from buffer get a batch of data\n",
        "    -prefetch one datapoint from the batch each time in order to iterative process data\n",
        "    all above are actions, thus we can see in train stage, it initilize a iterator \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    filenames = [os.path.join(FLAGS['tf_records_train_path'], f) for f in os.listdir(FLAGS['tf_records_train_path'])]\n",
        "    \n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    #Creates a TFRecordDataset to read one or more TFRecord files.\n",
        "    dataset = dataset.map(parse)\n",
        "    #Maps map_func across the elements of this dataset.\n",
        "    #This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements,\n",
        "    #in the same order as they appeared in the input\n",
        "    dataset = dataset.shuffle(buffer_size=500) #Randomly shuffles a tensor along its first dimension.\n",
        "                                               #buffer_size representing the number of elements from this dataset from which the new dataset will sample.\n",
        "    dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.batch(FLAGS['batch_size'])\n",
        "    #Combines consecutive elements of this dataset into batches.\n",
        "    dataset = dataset.prefetch(buffer_size=1)\n",
        "    #Creates a Dataset that prefetches elements from this dataset.\n",
        "    \n",
        "    '''\n",
        "    dataset 2 is used to validation, here called infer\n",
        "    shuffle with buffer size 1 and batch with size 1 is because for validation, we only need one datapoint each time\n",
        "    to get corresponding prediction\n",
        "    but for train, we use batch train to speed up\n",
        "    '''\n",
        "    dataset2 = tf.data.TFRecordDataset(filenames)\n",
        "    dataset2 = dataset2.map(parse)\n",
        "    dataset2 = dataset2.shuffle(buffer_size=1)\n",
        "\n",
        "    dataset2 = dataset2.repeat()\n",
        "    dataset2 = dataset2.batch(1)\n",
        "    dataset2 = dataset2.prefetch(buffer_size=1)\n",
        "\n",
        "    return dataset, dataset2\n",
        "\n",
        "\n",
        "def _get_test_data(FLAGS):\n",
        "    ''' Buildind the input pipeline for test data.'''\n",
        "\n",
        "    filenames = [os.path.join(FLAGS['tf_records_test_path'], f) for f in os.listdir(FLAGS['tf_records_test_path'])]\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(parse)\n",
        "    dataset = dataset.shuffle(buffer_size=1)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(1)\n",
        "    dataset = dataset.prefetch(buffer_size=1)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def parse(serialized):\n",
        "    ''' Parser for the TFRecords file.'''\n",
        "\n",
        "    features = {'movie_ratings':tf.FixedLenFeature([3952], tf.float32),  \n",
        "              }\n",
        "    parsed_example = tf.parse_single_example(serialized,\n",
        "                                           features=features,\n",
        "                                           )\n",
        "    movie_ratings = tf.cast(parsed_example['movie_ratings'], tf.float32)\n",
        "    \n",
        "    return movie_ratings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7CRV052qCZu",
        "colab_type": "code",
        "outputId": "123297c6-ee79-43c2-f711-2d452e74cecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "filenames = [os.path.join(FLAGS['tf_records_train_path'], f) for f in os.listdir(FLAGS['tf_records_train_path'])]\n",
        "dataset = tf.data.TFRecordDataset(filenames)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7a5dc8b6f527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf_records_train_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tf_records_train_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BJJbExzqF6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.map(parse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDHZL9hXBIjF",
        "colab_type": "text"
      },
      "source": [
        "## Define autoencoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iMfWX6RA1YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# import model_helper\n",
        "\n",
        "\n",
        "def _get_bias_initializer():\n",
        "    return tf.zeros_initializer()\n",
        "\n",
        "\n",
        "def _get_weight_initializer():\n",
        "    return tf.random_normal_initializer(mean=0.0, stddev=0.05)\n",
        "\n",
        "\n",
        "class DAE: #Data Acquisition Engine\n",
        "    \n",
        "    def __init__(self, FLAGS):\n",
        "        ''' Implementation of deep autoencoder class.'''\n",
        "        \n",
        "        self.FLAGS = FLAGS\n",
        "        self.weight_initializer = _get_weight_initializer()\n",
        "        self.bias_initializer = _get_bias_initializer()\n",
        "        self.init_parameters()\n",
        "        \n",
        "\n",
        "    def init_parameters(self):\n",
        "        '''Initialize networks weights and biasis.'''\n",
        "        \n",
        "        with tf.name_scope('weights'):\n",
        "          #This context manager validates that the given values are from the same graph,  \n",
        "          #makes that graph the default graph, and pushes a name scope in that graph\n",
        "            self.W_1 = tf.get_variable(name='weight_1', shape=(self.FLAGS['num_v'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer) #Gets an existing variable with these parameters or create a new one\n",
        "            self.W_2 = tf.get_variable(name='weight_2', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_3 = tf.get_variable(name='weight_3', shape=(self.FLAGS['num_h'], self.FLAGS['num_h']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            self.W_4 = tf.get_variable(name='weight_4', shape=(self.FLAGS['num_h'], self.FLAGS['num_v']),\n",
        "                                       initializer=self.weight_initializer)\n",
        "            # self.W_5 = tf.get_variable(name='weight_5', shape=(self.FLAGS['num_h'], self.FLAGS['num_v']),\n",
        "            #                            initializer=self.weight_initializer)\n",
        "            \n",
        "        with tf.name_scope('biases'):\n",
        "            self.b1 = tf.get_variable(name='bias_1', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b2 = tf.get_variable(name='bias_2', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            self.b3 = tf.get_variable(name='bias_3', shape=(self.FLAGS['num_h']),\n",
        "                                      initializer=self.bias_initializer)\n",
        "            # self.b4 = tf.get_variable(name='bias_3', shape=(self.FLAGS['num_h']),\n",
        "            #                           initializer=self.bias_initializer)\n",
        "    def _inference(self, x):\n",
        "        ''' Making one forward pass. Predicting the networks outputs.\n",
        "        @param x: input ratings\n",
        "        \n",
        "        @return : networks predictions\n",
        "        '''\\\n",
        "        \n",
        "        with tf.name_scope('inference'):\n",
        "          a1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, self.W_1),self.b1)) # sign(W1T*X+b1)\n",
        "          a2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a1, self.W_2),self.b2))\n",
        "          a3 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a2, self.W_3),self.b3))\n",
        "          a4 = tf.matmul(a3, self.W_4)\n",
        "          # a4 = tf.nn.relu(tf.nn.bias_add(tf.matmul(a3, self.W_4), self.b4))   \n",
        "          # a5 = tf.matmul(a3, self.W_5)\n",
        "        return a4\n",
        "    \n",
        "    def _compute_loss(self, predictions, labels, num_labels):\n",
        "        ''' Computing the Mean Squared Error loss between the input and output of the network.\n",
        "            \n",
        "          @param predictions: predictions of the stacked autoencoder\n",
        "          @param labels: input values of the stacked autoencoder which serve as labels at the same time\n",
        "          @param num_labels: number of labels !=0 in the data set to compute the mean\n",
        "            \n",
        "          @return mean squared error loss tf-operation\n",
        "          '''\n",
        "            \n",
        "        with tf.name_scope('loss'):\n",
        "            loss_op = tf.div(tf.reduce_sum(tf.square(tf.subtract(predictions,labels))),num_labels)\n",
        "            return loss_op\n",
        "          \n",
        "        \n",
        "\n",
        "    def _optimizer(self, x):\n",
        "        '''Optimization of the network parameter through stochastic gradient descent.\n",
        "            \n",
        "            @param x: input values for the stacked autoencoder.\n",
        "            \n",
        "            @return: tensorflow training operation\n",
        "            @return: ROOT!! mean squared error\n",
        "        '''\n",
        "        \n",
        "        outputs = self._inference(x)\n",
        "\n",
        "        mask = tf.where(tf.equal(x, 0.0), tf.zeros_like(x), x) # indices of 0 values in the training set\n",
        "        # tf.zero_like : Creates a tensor with all elements set to zero.\n",
        "        # The condition tensor acts as a mask that chooses, based on the value at each element, \n",
        "        # whether the corresponding element / row in the output should be taken from x (if true) or y (if false).\n",
        "        num_train_labels = tf.cast(tf.count_nonzero(mask), dtype=tf.float32) # number of non zero values in the training set\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) # boolean mask\n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs)) # set the output values to zero if corresponding input values are zero\n",
        "\n",
        "\n",
        "        MSE_loss = self._compute_loss(outputs,x,num_train_labels)\n",
        "        \n",
        "        if self.FLAGS['l2_reg'] == True:\n",
        "            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) # Returns all variables created with trainable=True.\n",
        "            MSE_loss = MSE_loss +  self.FLAGS['lambda_'] * l2_loss\n",
        "        \n",
        "        train_op = tf.train.AdamOptimizer(self.FLAGS['learning_rate']).minimize(MSE_loss) #An Operation that updates the variables in var_list\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "\n",
        "        return train_op, RMSE_loss\n",
        "    \n",
        "    def _validation_loss(self, x_train, x_test):\n",
        "        ''' Computing the loss during the validation time.\n",
        "            \n",
        "          @param x_train: training data samples\n",
        "          @param x_test: test data samples\n",
        "            \n",
        "          @return networks predictions\n",
        "          @return root mean squared error loss between the predicted and actual ratings\n",
        "          '''\n",
        "        \n",
        "        outputs = self._inference(x_train) # use training sample to make prediction\n",
        "        mask = tf.where(tf.equal(x_test,0.0), tf.zeros_like(x_test), x_test) # identify the zero values in the test ste\n",
        "        num_test_labels = tf.cast(tf.count_nonzero(mask),dtype=tf.float32) # count the number of non zero values\n",
        "        bool_mask = tf.cast(mask,dtype=tf.bool) \n",
        "        outputs = tf.where(bool_mask, outputs, tf.zeros_like(outputs))\n",
        "    \n",
        "        MSE_loss = self._compute_loss(outputs, x_test, num_test_labels)\n",
        "        RMSE_loss = tf.sqrt(MSE_loss)\n",
        "            \n",
        "        return outputs, RMSE_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylCWKRtHCAyw",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR286YkxBgT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def train(FLAGS):\n",
        "    '''Building the graph, opening of a session and starting the training od the neural network.'''\n",
        "    \n",
        "    num_batches = int(FLAGS['num_samples']/FLAGS['batch_size'])\n",
        "\n",
        "    with tf.Graph().as_default():\n",
        "\n",
        "        train_data, train_data_infer = _get_training_data(FLAGS)\n",
        "        test_data = _get_test_data(FLAGS)\n",
        "        \n",
        "        iter_train = train_data.make_initializable_iterator()\n",
        "        #Creates a tf.data.Iterator for enumerating the elements of a dataset.\n",
        "        iter_train_infer = train_data_infer.make_initializable_iterator()\n",
        "        iter_test = test_data.make_initializable_iterator()\n",
        "        \n",
        "        x_train = iter_train.get_next() #Returns a nested structure of tf.Tensors representing the next element.\n",
        "        x_train_infer = iter_train_infer.get_next()\n",
        "        x_test = iter_test.get_next()\n",
        "\n",
        "        model = DAE(FLAGS)\n",
        "\n",
        "        train_op, train_loss_op = model._optimizer(x_train)\n",
        "        pred_op, test_loss_op = model._validation_loss(x_train_infer, x_test)\n",
        "       \n",
        "        with tf.Session() as sess: #A class for running TensorFlow operations\n",
        "            \n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            train_loss = 0\n",
        "            test_loss = 0\n",
        "\n",
        "            for epoch in range(FLAGS['num_epoch']):\n",
        "                \n",
        "                sess.run(iter_train.initializer) #The returned iterator will be in an uninitialized state, \n",
        "                                                 #and you must run the iterator.initializer operation before using it\n",
        "                \n",
        "                for batch_nr in range(num_batches):\n",
        "                    \n",
        "                    _, loss_ = sess.run((train_op, train_loss_op))\n",
        "                    train_loss += loss_\n",
        "              \n",
        "                sess.run(iter_train_infer.initializer)\n",
        "                sess.run(iter_test.initializer)\n",
        "\n",
        "                for i in range(FLAGS['num_samples']):\n",
        "                    pred, loss_ = sess.run((pred_op, test_loss_op))\n",
        "                    test_loss += loss_\n",
        "                    \n",
        "                print('epoch_nr: %i, train_loss: %.3f, test_loss: %.3f'%(epoch,(train_loss/num_batches), (test_loss/FLAGS['num_samples'])))\n",
        "                train_loss = 0\n",
        "                test_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFghgRSbFpoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = {'tf_records_train_path': '/content/autoencoder/data/ml-1m/train/',  # Path of the training data\n",
        "         'tf_records_test_path': '/content/autoencoder/data/ml-1m/test/',  # Path of the test data\n",
        "         'num_epoch': 100,  # Number of training epochs\n",
        "         'batch_size': 16,  # Size of the training batch\n",
        "         'learning_rate': 5e-4,  # Learning_Rate\n",
        "         'l2_reg': False,  # L2 regularization\n",
        "         'lambda_': 0.01,  # Wight decay factor\n",
        "         'num_v': 3952,  # Number of visible neurons (Number of movies the users rated.\n",
        "         'num_h': 128,  # Number of hidden neurons\n",
        "         'num_samples': 5953}  # Number of training samples (Number of users, who gave a rating)\n",
        "\n",
        "\n",
        "train(FLAGS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm-AYoNRPBvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}